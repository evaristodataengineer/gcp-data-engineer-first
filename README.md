<div align="center">

# üöÄ Google Cloud Platform - Portfolio de Data Engineering

[![GCP](https://img.shields.io/badge/Google_Cloud-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)](https://cloud.google.com/)
[![BigQuery](https://img.shields.io/badge/BigQuery-669DF6?style=for-the-badge&logo=google-bigquery&logoColor=white)](https://cloud.google.com/bigquery)
[![Dataflow](https://img.shields.io/badge/Dataflow-4285F4?style=for-the-badge&logo=google-cloud&logoColor=white)](https://cloud.google.com/dataflow)
[![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)](https://www.python.org/)
[![Apache Beam](https://img.shields.io/badge/Apache_Beam-FF6600?style=for-the-badge&logo=apache&logoColor=white)](https://beam.apache.org/)

### üìä Portafolio completo de proyectos con Google Cloud Platform para demostrar competencias como Data Engineer

*Una colecci√≥n integral de proyectos pr√°cticos que demuestran dominio en el ecosistema de GCP, desde ingesta y transformaci√≥n de datos hasta arquitecturas avanzadas de Data Mesh y ML*

[Ver Proyectos](#-proyectos) ‚Ä¢ [Tecnolog√≠as](#-stack-tecnol√≥gico) ‚Ä¢ [Contacto](#-contacto)

---

</div>

## üéØ Sobre este Repositorio

Este repositorio contiene una **suite completa de proyectos** desarrollados con **Google Cloud Platform**, dise√±ados para demostrar habilidades profesionales en **Ingenier√≠a de Datos**. Cada carpeta representa un √°rea espec√≠fica del ecosistema GCP, con implementaciones reales y casos de uso empresariales.

### üí° ¬øPor qu√© este repositorio?

- ‚úÖ **Experiencia pr√°ctica** con servicios clave de GCP
- ‚úÖ **Casos de uso reales** aplicables a entornos de producci√≥n
- ‚úÖ **Mejores pr√°cticas** en arquitectura de datos en la nube
- ‚úÖ **C√≥digo documentado** y reutilizable
- ‚úÖ **Enfoque en empleabilidad** como Data Engineer

---

## üõ† Stack Tecnol√≥gico

<table>
<tr>
<td align="center" width="25%">
<img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/googlecloud/googlecloud-original.svg" width="60" height="60" />
<br><b>Google Cloud</b>
</td>
<td align="center" width="25%">
<img src="https://www.vectorlogo.zone/logos/apache_beam/apache_beam-icon.svg" width="60" height="60" />
<br><b>Apache Beam</b>
</td>
<td align="center" width="25%">
<img src="https://cdn.jsdelivr.net/gh/devicons/devicon/icons/python/python-original.svg" width="60" height="60" />
<br><b>Python</b>
</td>
<td align="center" width="25%">
<img src="https://www.vectorlogo.zone/logos/apache_airflow/apache_airflow-icon.svg" width="60" height="60" />
<br><b>Apache Airflow</b>
</td>
</tr>
</table>

### Servicios de GCP Utilizados:
- **BigQuery** - Almacenamiento y an√°lisis de datos a escala masiva
- **Cloud Storage** - Almacenamiento de objetos y Data Lake
- **Dataflow** - Procesamiento de datos en streaming y batch con Apache Beam
- **Cloud Composer** - Orquestaci√≥n de flujos de trabajo con Apache Airflow
- **Dataplex** - Gesti√≥n unificada de datos y Data Mesh
- **Cloud SQL** - Bases de datos relacionales gestionadas
- **Dataproc** - Clusters de Spark/Hadoop gestionados
- **Cloud Monitoring** - Observabilidad y troubleshooting

---

## üìÇ Proyectos

### 1Ô∏è‚É£ **BigQuery - Analytics & Data Warehousing** 
üìÅ [`Big-Query/`](./Big-Query) | [`Bigquery-model/`](./Bigquery-model)

Implementaci√≥n de un Data Warehouse moderno con BigQuery, desde consultas b√°sicas hasta modelos de datos complejos.

**Caracter√≠sticas:**
- üîç Consultas sobre datasets p√∫blicos de BigQuery
- üìä Dise√±o de esquemas de datos (users, orders)
- üéØ Vistas desnormalizadas para analytics
- üíæ Inserci√≥n y gesti√≥n de datos
- üîó Joins y agregaciones optimizadas

**Tecnolog√≠as:** BigQuery, SQL, Python SDK

**Casos de uso:**
- An√°lisis de e-commerce
- Reporting de ventas
- KPIs de negocio

```sql
-- Ejemplo: Vista desnormalizada para an√°lisis
CREATE OR REPLACE VIEW ecommerce_dataset.denormalized_view AS
SELECT
    o.user_id, o.order_id, o.product, o.price,
    u.name, u.email
FROM ecommerce_dataset.orders o
JOIN ecommerce_dataset.users u
ON o.user_id = u.user_id;
```

---

### 2Ô∏è‚É£ **Cloud Storage - Object Storage & Data Lake**
üìÅ [`Cloud-storage/`](./Cloud-storage) | [`Changes/`](./Changes)

Gesti√≥n program√°tica de Cloud Storage para la creaci√≥n de Data Lakes y almacenamiento escalable.

**Caracter√≠sticas:**
- ü™£ Creaci√≥n autom√°tica de buckets
- üì§ Upload/Download de archivos
- üîß Configuraci√≥n de permisos y lifecycle policies
- üìä Integraci√≥n con BigQuery para datos externos
- üîÑ Procesamiento de archivos CSV

**Tecnolog√≠as:** Cloud Storage, Python SDK

**Casos de uso:**
- Almacenamiento de datos raw
- Archivado de logs
- Data Lakes multi-zona

```python
# Ejemplo: Creaci√≥n de bucket con configuraci√≥n
def create_bucket(bucket_name, location="US"):
    client = storage.Client()
    bucket = client.bucket(bucket_name)
    client.create_bucket(bucket, location=location)
    print(f"üü© Bucket creado: {bucket.name}")
```

---

### 3Ô∏è‚É£ **Dataflow - Stream & Batch Processing**
üìÅ [`dataflow/`](./dataflow)

Pipelines de procesamiento de datos usando Apache Beam en Google Dataflow para transformaciones ETL a escala.

**Caracter√≠sticas:**
- üåä Procesamiento batch con Apache Beam
- üìù Pipeline WordCount cl√°sico
- ‚òÅÔ∏è Ejecuci√≥n en Dataflow Runner
- üéØ Transformaciones PTransform
- üíæ Escritura a Cloud Storage

**Tecnolog√≠as:** Apache Beam, Dataflow, Python

**Casos de uso:**
- ETL de archivos de texto
- Procesamiento de logs
- An√°lisis de datos no estructurados

```python
# Pipeline de procesamiento con Apache Beam
with beam.Pipeline(options=options) as p:
    (
        p
        | "Leer archivo" >> beam.io.ReadFromText("gs://input/data.txt")
        | "Separar palabras" >> beam.FlatMap(lambda line: line.split())
        | "Contar palabras" >> beam.combiners.Count.PerElement()
        | "Guardar resultados" >> beam.io.WriteToText("gs://output/wordcount")
    )
```

---

### 4Ô∏è‚É£ **Cloud Composer - Workflow Orchestration**
üìÅ [`Composer/`](./Composer) | [`composer_dags/`](./composer_dags)

Orquestaci√≥n de flujos de trabajo complejos usando Cloud Composer (Apache Airflow managed).

**Caracter√≠sticas:**
- üîÑ DAGs de Airflow para orquestaci√≥n
- üöÄ Integraci√≥n con Dataflow Templates
- ‚è∞ Scheduling y retries
- üìä Monitoreo de ejecuciones
- üîó Operadores de GCP

**Tecnolog√≠as:** Cloud Composer, Apache Airflow, Dataflow

**Casos de uso:**
- ETL programados
- Pipelines de ML
- Orquestaci√≥n multi-servicio

```python
# DAG de Airflow para ejecutar Dataflow
dataflow_task = DataflowTemplatedJobStartOperator(
    task_id='ejecutar_wordcount',
    template='gs://bucket/templates/wordcount_template',
    location='us-central1',
    project_id='mi-proyecto',
    dag=dag,
)
```

---

### 5Ô∏è‚É£ **Datalake - External Tables & Lakehouse**
üìÅ [`Datalake/`](./Datalake)

Implementaci√≥n de un Data Lake moderno con tablas externas en BigQuery apuntando a Cloud Storage.

**Caracter√≠sticas:**
- üèóÔ∏è Arquitectura Lakehouse (Storage + BigQuery)
- üìÅ Tablas externas sobre CSV en GCS
- üîç Consultas SQL sobre archivos raw
- üöÄ Separaci√≥n de storage y compute
- üí∞ Optimizaci√≥n de costos

**Tecnolog√≠as:** BigQuery, Cloud Storage, Bash

**Casos de uso:**
- Query directo sobre archivos
- Data Lake analytics
- Reducci√≥n de costos de almacenamiento

---

### 6Ô∏è‚É£ **Data Mesh - Decentralized Architecture**
üìÅ [`datamesh/`](./datamesh)

Implementaci√≥n de una arquitectura Data Mesh usando Dataplex para dominios de datos descentralizados.

**Caracter√≠sticas:**
- üèõÔ∏è Creaci√≥n de Data Lakes con Dataplex
- üåê Zonas de datos por dominio (ventas, marketing, log√≠stica)
- üîó Assets vinculados a Cloud Storage
- üì¶ Automatizaci√≥n con Python SDK
- üéØ Domain-driven data ownership

**Tecnolog√≠as:** Dataplex, Cloud Storage, Python

**Casos de uso:**
- Arquitecturas Data Mesh empresariales
- Gesti√≥n federada de datos
- Data governance distribuido

```python
# Crear zona de datos en Dataplex
def create_zone(project_id, region, lake_name, domain, bucket_name):
    client = dataplex_v1.DataplexServiceClient()
    zone = dataplex_v1.Zone()
    zone.display_name = f"{domain.capitalize()} Zone"
    zone.type_ = dataplex_v1.Zone.Type.RAW
    # ... configuraci√≥n y creaci√≥n
```

---

### 7Ô∏è‚É£ **Analysis Preparation - Materialized Views**
üìÅ [`analysis_preparation/`](./analysis_preparation)

Preparaci√≥n de datos para an√°lisis con vistas materializadas y agregaciones pre-computadas.

**Caracter√≠sticas:**
- üìä Materialized Views en BigQuery
- ‚ö° Agregaciones diarias y semanales
- üîÑ Refresh autom√°tico
- üíπ KPIs pre-calculados
- üéØ Optimizaci√≥n de queries anal√≠ticas

**Tecnolog√≠as:** BigQuery, SQL

**Casos de uso:**
- Dashboards de BI
- Reportes ejecutivos
- Analytics en tiempo real

```sql
-- Vista materializada para agregaciones diarias
CREATE MATERIALIZED VIEW ecommerce_analysis.orders_daily_mv AS
SELECT
  DATE(order_date) AS order_day,
  COUNT(order_id) AS total_orders,
  SUM(price) AS total_revenue
FROM ecommerce_analysis.orders_raw
GROUP BY order_day;
```

---

### 8Ô∏è‚É£ **ML Preparation - Feature Engineering**
üìÅ [`ml_prep/`](./ml_prep)

Preparaci√≥n de datos para Machine Learning con transformaciones de features y exploraci√≥n de datos.

**Caracter√≠sticas:**
- ü§ñ Feature engineering en BigQuery
- üîç Queries de exploraci√≥n de datos
- üìà Creaci√≥n de variables derivadas
- üéØ Preparaci√≥n para BigQuery ML
- üìä An√°lisis estad√≠stico

**Tecnolog√≠as:** BigQuery, SQL

**Casos de uso:**
- Preparaci√≥n de datasets para ML
- Feature stores
- An√°lisis exploratorio de datos

---

### 9Ô∏è‚É£ **Optimization - Performance & Cost**
üìÅ [`optimization/`](./optimization)

T√©cnicas de optimizaci√≥n para mejorar el rendimiento y reducir costos en GCP.

**Caracter√≠sticas:**
- üîß Particionamiento de tablas en BigQuery
- üí∞ Clusters bajo demanda con Dataproc
- ‚ö° Query optimization
- üìä An√°lisis de costos
- üéØ Reservations y slots

**Tecnolog√≠as:** BigQuery, Dataproc, Bash

**Casos de uso:**
- Reducci√≥n de costos de queries
- Optimizaci√≥n de pipelines
- Mejora de tiempos de respuesta

---

### üîü **Fault Tolerance - High Availability**
üìÅ [`fault_tolerance/`](./fault_tolerance)

Implementaci√≥n de estrategias de tolerancia a fallos y alta disponibilidad.

**Caracter√≠sticas:**
- üåç Multi-region en BigQuery
- üîÑ Replicaci√≥n de Cloud SQL
- üí™ Dataflow resiliente
- üõ°Ô∏è Backup y disaster recovery
- üìä Monitoreo de failover

**Tecnolog√≠as:** BigQuery, Cloud SQL, Dataflow, Bash

**Casos de uso:**
- Continuidad de negocio
- Sistemas cr√≠ticos 24/7
- Disaster recovery

---

### 1Ô∏è‚É£1Ô∏è‚É£ **Monitoring & Troubleshooting**
üìÅ [`monitoring_troubleshooting/`](./monitoring_troubleshooting)

Observabilidad y diagn√≥stico de sistemas de datos en GCP.

**Caracter√≠sticas:**
- üìä Dashboards de Cloud Monitoring
- üîç Logs y m√©tricas
- üö® Alertas automatizadas
- üìà Visualizaci√≥n de KPIs
- üõ†Ô∏è Troubleshooting proactivo

**Tecnolog√≠as:** Cloud Monitoring, Cloud Logging

**Casos de uso:**
- Monitoreo de pipelines
- Alertas de SLA
- Performance monitoring

---

### 1Ô∏è‚É£2Ô∏è‚É£ **Workload Organization - Resource Management**
üìÅ [`workload_organizate/`](./workload_organizate)

Organizaci√≥n y gesti√≥n de cargas de trabajo con reservations y slot management.

**Caracter√≠sticas:**
- üìÖ BigQuery reservations
- üéØ Slot assignment
- üíº Gesti√≥n de recursos
- üìä Optimizaci√≥n de workloads
- üí∞ Control de costos por equipo

**Tecnolog√≠as:** BigQuery, Bash

**Casos de uso:**
- Multi-tenancy
- Control presupuestario
- Aislamiento de workloads

---

### 1Ô∏è‚É£3Ô∏è‚É£ **Data Sharing - Cross-Organization**
üìÅ [`sharing_data/`](./sharing_data)

Compartir datos de forma segura entre proyectos y organizaciones.

**Caracter√≠sticas:**
- üîê Authorized views
- üåê Cross-project queries
- üë• IAM y permisos granulares
- üìä Analytics Hub
- üîó Data sharing controlado

**Tecnolog√≠as:** BigQuery, IAM

**Casos de uso:**
- Data marketplace
- Colaboraci√≥n entre equipos
- Data monetization

---

## üéì Competencias Demostradas

Este portafolio demuestra competencia profesional en:

### üèóÔ∏è **Arquitectura de Datos**
- Dise√±o de Data Lakes y Data Warehouses
- Implementaci√≥n de arquitecturas Data Mesh
- Lakehouse h√≠bridos (Storage + Compute)

### ‚öôÔ∏è **Ingenier√≠a de Datos**
- Pipelines ETL/ELT escalables
- Stream y batch processing
- Orquestaci√≥n de workflows

### üìä **Analytics Engineering**
- Modelado de datos dimensional
- Vistas materializadas y agregaciones
- Optimizaci√≥n de queries

### üîß **DevOps & Infraestructura**
- Infrastructure as Code
- Automatizaci√≥n con Python
- CI/CD de pipelines de datos

### üí∞ **Optimizaci√≥n & Costos**
- Performance tuning
- Cost optimization strategies
- Resource management

### üõ°Ô∏è **Reliability & Security**
- Fault tolerance
- High availability
- Data governance

---

## üöÄ C√≥mo Ejecutar los Proyectos

### Prerrequisitos

1. **Cuenta de Google Cloud Platform** con proyecto activo
2. **gcloud CLI** instalado y configurado
3. **Python 3.8+** instalado
4. **Permisos** adecuados en GCP (BigQuery Admin, Storage Admin, etc.)

### Configuraci√≥n Inicial

```bash
# 1. Clonar el repositorio
git clone https://github.com/tu-usuario/gcp-data-engineer-first.git
cd gcp-data-engineer-first

# 2. Autenticarse en GCP
gcloud auth login
gcloud config set project TU-PROJECT-ID

# 3. Instalar dependencias de Python
pip install google-cloud-bigquery google-cloud-storage apache-beam[gcp]

# 4. Configurar variables de entorno
export PROJECT_ID="tu-project-id"
export REGION="us-central1"
```

### Ejecutar Proyectos Espec√≠ficos

Cada carpeta contiene scripts ejecutables. Ejemplos:

```bash
# BigQuery - Consultar dataset p√∫blico
python Big-Query/public_dataset_demo.py

# Cloud Storage - Crear bucket
python Cloud-storage/create_bucket_script.py

# Dataflow - Ejecutar pipeline
python dataflow/pipeline_wordcount.py

# Datamesh - Crear arquitectura
python datamesh/create_zones_new.py \
  --project_id mi-proyecto \
  --region us-central1 \
  --lake retail-lake \
  --domains ventas marketing
```

---

## üìà Pr√≥ximos Pasos

Este repositorio est√° en **evoluci√≥n continua**. Futuras adiciones incluir√°n:

- [ ] **BigQuery ML** - Modelos de ML directamente en BigQuery
- [ ] **Pub/Sub + Dataflow** - Streaming en tiempo real
- [ ] **dbt (data build tool)** - Transformaciones declarativas
- [ ] **Terraform** - Infrastructure as Code
- [ ] **CI/CD con Cloud Build** - Automatizaci√≥n completa

---

## üìû Contacto

**Evaristo - Data Engineer**

[![LinkedIn](https://img.shields.io/badge/LinkedIn-0077B5?style=for-the-badge&logo=linkedin&logoColor=white)](https://www.linkedin.com/in/evaristo-sandoval-gil-86a6a0291/)
[![GitHub](https://img.shields.io/badge/GitHub-100000?style=for-the-badge&logo=github&logoColor=white)](https://github.com/evaristodataengineer)
[![Email](https://img.shields.io/badge/Email-D14836?style=for-the-badge&logo=gmail&logoColor=white)](mailto:evaristodataengineer@gmail.com)

> üíº **En b√∫squeda activa de oportunidades como Data Engineer**


<div align="center">

### ‚≠ê Si este repositorio te resulta √∫til, considera darle una estrella

**Hecho con ‚ù§Ô∏è para la comunidad de Data Engineering**

</div>



